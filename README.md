# hackai_moscow_top_news
predict the main themes for different target audiences. 
https://hacks-ai.ru/championships/758453






Оглавление:
- Описание
- Итог
- Структура директорий проекта
- Выполнение




 ОПИСАНИЕ


 Основная идея, заложенная в решение проекта - выявление паттернов повседневного
 поведения пользователей, как в классических алгоритмах предсказания временных 
 рядов вроде SARIMAX: они читают статьи когда едут на работу, в будни, как ведут 
 себя в районе праздников. Однако, по представленным данным невозможно было
 определить даже годоваые паттерны, не говоря уже о регулярных, более редких, 
 событиях вроде Олимпиады, ежегодных ЧМ по хоккею и пр. Так же невозможно только 
 на основе таких паттернов было предсказать значения для статей о COVID, Украине, 
 санкциях и пр. Вот для такого уточнения уже предполагалось воспользоваться NLP и
 работой с содержанием заголовков / текстов.
 
 При первичноv анализе данных было выявлено разное распределение данных для 
 целевых значений в датах до ~2022-04-08 и после: в первую очередь для Depth и
 Views. Статистически гипотеза не проверялась. Предположу, что данные до 
 2022-04-08 и после это данные со стационарных пк и мобильных устройств.
 Строить одну модель для визуально разных распределений представлялось не 
 разумным и было выбрано направление с построением разных моделей для статей
 с датой 2022-04-08 и после. Так же присутствуют явно разные распределения для
 каждой категории, однако для  построения отдельной модели для каждой категории
 представленных даннных недостаточно. 
 Так же, предположительно из других распределений (распределений с другими 
 параметрами) были статьи из обучающей выборки за 17гг и 18гг. Они были удалены
 из обучающей выборки. 
 В дополнении выло выявлено аномальное поведение целевых переменных у статей с
 признаком ctr = 6.096. Они были полностью одинаковы. Такие статьи так же были 
 удалены из обучающей выборки, а в тестовой в статьях с ctr = 6.096 значения
 целевых переменных заменялись константами (всего 3 таких статьи).
 Так еж были удалены статьи с 3 категориями для которых присутствовали только
 по одной статье в обучающей выборке и не было статей в тестовой.
 В дополнении были убраны статьи из тестовой выбоки с Full_reads_percent >100.
 Больше никакой очистки данных в части удаления данных с итоговом решении не
 проводилось.
 
 Одной из первых была выдвинута гипотеза: длина текста существенно влияет на
 целевые переменные Depth и Full_reads_percent. Для получения длинны текста 
 был проведен парсинг статей с РКБ. Из спарсенных статей извлекались параметры:
 длина текста, количество фотографий нем.
 
 Следующим этапом было извлечение статистических данных. Да, определить тренды 
 и годовые паттерны мы не можем, однако они по прежнему присутствуют и это 
 невозможно игнорировать. Были собраны статистические данные (мин, макс, среднее,
 среднеквадратичное) по часам, дням недели, месяцам, праздничным, 
 предпраздничным и послепраздничным дням. В некоторых случаях собирались лаги и
 разница зачений в период до 7 дней. В дополнение бырались скользящие окна за
 2, 3 и 7 дней с весами согласно распределению гауса. Предполагал, что это
 будет соответствовать краткосрочным трендам. Статистики собирались раздельно 
 для дат до 2022-04-08 и после.
 
 Следующим этапом собирались статистики по авторам и по категориям. Гипотеза в 
 том, что не всем пользователям интересны все категории и темы, на которые пищут 
 авторы. Собирались статистики авторов (мин, макс, среднее, среднеквадратичное)
 и ститистики категорий (мин, макс, среднее, среднеквадратичное, лаги и разница
 до 7 дней, скользящее окно за 2, 3 и 7 дней с весами по распределению гауса).
 Статистики для авторов собирались без разделения дат. Если у статьи было 
 несколько авторов то брались средние значения.
 Статистики категорий собирались раздельно для дат до 2022-04-08 и после. 
 В дополнение был введен признак количества авторов статьи.
 Авторы и категории так же преобразовывались в коды категории так же, как и
 в безлайне.
 
 Таким же образом преобразовывались и тэги. В дополнение был введен признак
 количества тегов у статьи.
 
 Часть из числовых признаков пропускалось через математические операции вроде 
 квадратного корнчя и т.п.
 
 Заголовок статьи преобразовывался в эмбеддинги через модель sbert_large_mt_nlu_ru
 от sberbank-ai. В целях борьбы с перебучением вектора эмбеддингов затем 
 ужимались до вектора длинной 64 через PCA. В дополнение из заголовка брались
 бинарные признаки: прямая трансляция, инфорграфика, фтоторепортаж и т.п.

 
 Моделей предполагал строить 6: для 3 целевых переменных с разделением дат 
 до 2022-04-08 и после. Брал модели Catgboost, Xgboost и lgbm. 
 Выбирал лучшее количество итераций по cv по минимальной средней RMSE на 
 валидации в cv. Дальше обучал молель с выбранным количеством итераций. Лучшую 
 модель выбирал по минимальной RMSE на валидации в CV. 
 Ею оказался Catboost. 
 Ансамбль 3х моделей не зашел, сильно переобучался: на валидации в cv R2 
 было ~0.95. Что на паблике уменьшало скор чистого Catboost на ~0.02.  
 
 
 
 
 ИТОГ
 
 паблик 0.751067 и 14 место
 приват 0.748285 и 8 место

 ек
 views до 2022-04-08:
 sqrt(crt) и crt
 text_len и (1 / text_len^2)
 hour
 эмбеддинги заголовка
 
 views после 2022-04-08:
 crt и sqrt(crt) 
 text_len и (1 / text_len^2)
 hour
 эмбеддинги заголовка
   
 
 depth до 2022-04-08:
 среднее depth автора
 1 / text_len^2
 sqrt(text_len)
 -1 / (количество авторов)^2 и 1 / (количество авторов)^2
 эмбеддинги заголовка
 
 depth после 2022-04-08:
 hour
 среднее depth 
 среднее views
 прямая трансляция
 эмбеддинги заголовка
 

 full_reads_percent до 2022-04-08:
 text_len и 1 / text_len^2
 среднее frp по автору
 среднее frp по часу 
 эмбеддинги заголовка
 
 full_reads_percent после 2022-04-08:
 1 / text_len^2 и sqrt(text_len)
 среднее frp по автору
 ctr и sqrt(ctr)
 эмбеддинги заголовка




СТРУКТУРА ДИРЕКТОРИЙ ПРОЕКТА

═╦══data══╦══train.csv
 ║        ║
 ║        ╠══test.csv
 ║        ║
 ║        ╚══pages══*.html
 ║
 ╠══models══
 ║
 ╚══subm══╦══
          ║
          ╚══partial═
 
 в data находятся изначальные train и test.csv.
 в нее же будут добавляться различные промежуточные файлы,
 исользуемые при построении признаков и моделей.
 
 в pages хранятся все загруженные статьи.
 их можно загрузить самостоятельно, откомментировав блоки в 2step_rbk_parse.ipynb, 
 начинающиеся с 'load_number = 0'
 однако это занимает долгое время (с учетом защиты от блокировки),
 так что лучше все html загрузить сразу архивом с gdrive
 https://drive.google.com/file/d/1buUc2sJdVigoHu2QrtADblHui2CF35f1/view?usp=sharing
  
 в models сохраняются все обученные модели.
 
 в subm сохраняются все предсказания.
 
 в partial сохраняются предсказания, необходимые для обучения ансамбля 
 (не обучается в итоге)




 ВЫПОЛНЕНИЕ
  
 
 В первую очередь хочу обратить внимание, что проект выполнялся на старых версиях библиотек:
 sklearn          : 0.24.2
 numpy            : 1.20.3
 pandas           : 0.25.3
 С последними версиями известны проблеммы совместимости в названии атрибутов.
 Ведется работа над решением данного вопроса. Пока в процессе.
  
 0. Все ячейки в файлах выполняются последовательно сверху вниз;
 1. Первым этапом идет загрузка данных. Лучше выполнить, скачав архив с gdrive 
    https://drive.google.com/file/d/1buUc2sJdVigoHu2QrtADblHui2CF35f1/view?usp=sharing
    и распаковав в ./data/pages/. 
    Однако можно и загрузить самостоятельно, откомментировав блоки, начинающиеся с 
    'load_number = 0' в файле 2step_rbk_parse.ipynb;
 2. Далее идет полное выполнение 2step_rbk_parse.ipynb. где из загруженных в п.1 файлов 
    собирается информация (длинна текста, количество изображений в статье и пр.) и сохраняется
    в расширенных файлах train_extended.csv и test_extended.csv;
 3. Затем выполняется файл 2step_embedings_titile.ipynb. в нем заголовки преобразуются в 
    эмбединги (sberbank-ai/sbert_large_mt_nlu_ru) после чего они пережимаются через pca до 
    вектора длинной 64;
 4. Выполняется файл 2step_make_feauturesюшзнти в котором происходит очистка данных,
    формируются статистики (min, max и т.д.) и ряд других признаков;
 5. Выполняется файл 3step_modeling_catboost.ipynb котором происходит обучение модели и построение предсказания. 
    Предсказание (файл сабмита) будет сохранен в директории subm;
 
